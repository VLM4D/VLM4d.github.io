<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VLM4D: Towards Spatiotemporal Awareness in Vision Language Models">
  <meta name="keywords" content="VLM, spatiotemporal reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="PvilQQmOfgqfpL57j2H43EAQ1-rlZQdgZ7Wrp6cnIYk" />
  <title>VLM4D: Towards Spatiotemporal Awareness in Vision Language Models</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">
</head>
<body>



  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon.png">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- <script src="https://assets.crowd.aws/crowd-html-elements.js"></script> -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">



</section>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VLM4D: Towards Spatiotemporal Awareness in Vision Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ShijieZhou-UCLA.github.io/">Shijie Zhou</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://asvilesov.github.io/">Alexander Vilesov</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://sheehan1230.github.io/">Xuehai He</a><sup>2,3*</sup>,</span>
            <span class="author-block">
              <a href="http://raywzy.com/">Ziyu Wan</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href=" ">Shuwang Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href=" ">Aditya Nagachandra</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href=" ">Di Chang</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://www.dongdongchen.bid/">Dongdong Chen</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://eric-xw.github.io/">Xin Eric Wang</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://samueli.ucla.edu/people/achuta-kadambi/">Achuta Kadambi</a><sup>1</sup>,</span>
          </div>
          
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">* Equal Contribution</span>
          </div> -->
          <style>
            .univerity-block {
                margin-right: 14px;
            }
          </style>
          <div class="is-size-5 publication-authors">
            <span class="univerity-block"><sup>1</sup>UCLA</span>
            <span class="univerity-block"><sup>2</sup>Microsoft</span>
            <span class="univerity-block"><sup>3</sup>UCSC</span>
            <span class="univerity-block"><sup>4</sup>USC</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block">*Denotes Equal Contribution</span>
          </div>
          
        </div>
        
      </div>
    </div>
    <section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <br>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->

              <span class="link-block">
                <a href="file/VLM4D.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/shijiezhou/VLM4D" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

              <span class="link-block">
                <a href="#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon has-text-white">
                    <i class="fa-solid fa-trophy"></i>
                      <!-- <p style="font-size:18px">üèÜ</p> -->
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>
    </a>
</span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  </div>
  
</section>






<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-language models (VLMs) have shown remarkable capabilities in integrating linguistic and visual reasoning but remain fundamentally limited in understanding dynamic spatiotemporal interactions. Humans effortlessly track and reason about object movements, rotations, and perspective shifts‚Äîabilities essential for robust real-world understanding yet notably lacking in current VLMs. In this paper, we introduce VLM4D, the first benchmark specifically designed to evaluate the spatiotemporal reasoning capabilities of VLMs. Our benchmark comprises diverse real-world and synthetic videos accompanied by carefully curated question-answer pairs emphasizing translational and rotational motions, perspective awareness, and motion continuity. Through comprehensive evaluations of state-of-the-art open and closed-source VLMs, we identify significant performance gaps compared to human baselines, highlighting fundamental deficiencies in existing models. Extensive analysis reveals that VLMs struggle particularly with integrating multiple visual cues and maintaining temporal coherence. We further explore promising directions, such as leveraging 4D feature field reconstruction and targeted spatiotemporal supervised fine-tuning, demonstrating their effectiveness in enhancing spatiotemporal comprehension. Our work aims to encourage deeper exploration into improving VLMs‚Äô spatial and temporal grounding, paving the way towards more capable and reliable visual intelligence for dynamic environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="7%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Spatiotemporal (4D) Awareness. </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">  
            <img id="model" width="80%" src="./static/images/figures/teaster_v5.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 1. Spatiotemporal (4D) Awareness. Humans intuitively reason in 4D (3D space + time), effortlessly reconstructing the dynamic spatial trajectory of moving objects from any perspective. In contrast, current Vision Language Models (VLMs) typically rely on aggregating 2D visual features across time, leading to incorrect predictions when motion understanding and interpretation requires deeper spatiotemporal reasoning. In this example, humans correctly perceive the car moving to the right, while the VLM (GPT-4o) inaccurately predicts leftward movement, suggesting VLMs struggle to perform spatiotemporal reasoning. </b></p>
            </h3>   
        </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Distribution of Dataset Sources and Annotations. </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">  
            <img id="model" width="30%" src="./static/images/figures/piechart_dataset_stats.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 2. Distribution of Dataset Sources and Annotations. Breakdown of our dataset illustrating the proportions of data sourced from third-person (Davis, YouTube), first-person (Ego4D), and synthetic data, categorized by annotation types: translational, rotational, action, counting, and false positives. </b></p>
            </h3>   
        </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Dataset Generation and Annotation</h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">  
            <img id="model" width="100%" src="./static/images/figures/dataset_generation_v2.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 3. Dataset Generation and Annotation Pipeline. Our dataset was constructed by collecting real videos and generating synthetic data, followed by human-in-the-loop quality reviews to address ambiguous videos and annotations. After temporal alignment and quality assurance, human-annotated questions and answers were created, complemented by multiple-choice questions generated by large language models (LLMs). The final dataset includes real-world and synthetic video data with comprehensive VLM scoring metrics. </b></p>
            </h3>   
        </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Qualitative Examples of Dataset Annotations. </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">     
            <img id="model" width="50%" src="./static/images/figures/dataset_qualitative_v3.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 4. (Top) A third-person video with translational annotations ("camel turning left from its perspective"). (Middle) A first-person video with a rotational question ("clockwise rotation of ladle"). (Bottom) A synthetic scene with action recognition "robotic dog moving left".</b></p>
            </h3>   


        </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Model Performance</h2> 
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-half">  
        <img id="model" width="100%" src="./static/images/figures/radar_plot.png">
      </div>
      <div class="column is-half">  
        <img id="model" width="100%" src="./static/images/figures/cot_vs_do.png">
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"><b>Figure 5. Model Accuracy Across Real Scene Question Categories of top-performing VLMs (left). Comparison of CoT and DO Accuracy Across Models (right).</b></p>
        </h3>   
      </div>
    </div>
  </div>
</section>






<section class="section" id="leaderboard">
  <div class="container is-max-desktop">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Leaderboard</h2>
        <div class="content">
          <div class="content has-text-justified">
            <p>
              Evaluation on VLM4D Benchmark across various proprietary and open-source VLMs.
            </p>
          </div>    
          <!-- Validation Set Leaderboard -->
          <!---- Updated scores sourced from the June 2025 LaTeX results -->
<table id="table1" class="js-sort-table">
  <thead>
    <tr>
      <th class="js-sort-string"><strong>Organization</strong></th>
      <th class="js-sort-string"><strong>Model</strong></th>
      <th class="js-sort-string"><strong>Release</strong></th>
      <th class="js-sort-number"><strong>Ego-centric</strong></th>
      <th class="js-sort-number"><strong>Exo-centric</strong></th>
      <th class="js-sort-number"><strong>Real&nbsp;Avg</strong></th>
      <th class="js-sort-number"><strong>Directional</strong></th>
      <th class="js-sort-number"><strong>FP</strong></th>
      <th class="js-sort-number"><strong>Synth&nbsp;Avg</strong></th>
      <th class="js-sort-number"><strong>Overall</strong></th>
    </tr>
  </thead>

    <tbody>
    <!-- 1 ‚Äì 10 -->
    <tr style="background-color:rgba(200,200,255,0.3);"><td>User&nbsp;Study</td><td>Human&nbsp;Performance</td><td></td><td>99.6</td><td>99.7</td><td>99.7</td><td>91.8</td><td>100.0</td><td>95.9</td><td>98.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Random</td><td>Random&nbsp;Selection</td><td></td><td>24.4</td><td>23.2</td><td>23.6</td><td>25.5</td><td>24.7</td><td>25.1</td><td>24.2</td></tr>
    <tr style="background-color:rgba(255,200,200,0.3);"><td>Google</td><td>Gemini-2.5-Pro</td><td>2025-03</td><td>68.2</td><td>70.5</td><td>69.7</td><td>71.3</td><td>75.0</td><td>71.6</td><td>70.2</td></tr>
    <tr style="background-color:rgba(255,200,200,0.3);"><td>Anthropic</td><td>Claude-3.7-Sonnet</td><td>2025-02</td><td>51.2</td><td>65.0</td><td>60.5</td><td>45.3</td><td>93.3</td><td>50.1</td><td>57.9</td></tr>
    <tr style="background-color:rgba(255,200,200,0.3);"><td>OpenAI</td><td>GPT-4o</td><td>2024-11</td><td>54.3</td><td>61.2</td><td>58.9</td><td>47.8</td><td>47.5</td><td>47.7</td><td>56.2</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Meta</td><td>Llama-4-Maverick-17B</td><td>2025-04</td><td>48.8</td><td>52.9</td><td>51.6</td><td>56.0</td><td>50.0</td><td>55.5</td><td>52.5</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Alibaba</td><td>Qwen2.5-VL-72B-AWQ</td><td>2025-01</td><td>49.9</td><td>48.7</td><td>49.1</td><td>54.3</td><td>75.0</td><td>56.1</td><td>50.8</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>OpenGVLab</td><td>InternVideo2.5-8B</td><td>2025-01</td><td>52.8</td><td>50.1</td><td>51.0</td><td>45.3</td><td>32.5</td><td>44.1</td><td>49.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Meta</td><td>Llama-4-Scout-17B</td><td>2025-04</td><td>46.6</td><td>51.3</td><td>49.7</td><td>46.8</td><td>45.0</td><td>46.6</td><td>49.0</td></tr>
    <tr style="background-color:rgba(255,200,200,0.3);"><td>xAI</td><td>Grok-2-Vision</td><td>2024-12</td><td>44.1</td><td>48.8</td><td>47.3</td><td>49.0</td><td>75.0</td><td>51.4</td><td>48.3</td></tr>
    <tr style="background-color:rgba(255,200,200,0.3);"><td>Google</td><td>Gemini-2.0-Pro</td><td>2025-02</td><td>44.8</td><td>50.5</td><td>48.7</td><td>42.8</td><td>52.5</td><td>43.6</td><td>47.4</td></tr>

    <!-- 11 ‚Äì 20 -->
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Shanghai&nbsp;AI&nbsp;Lab</td><td>InternVL2.5-38B</td><td>2024-11</td><td>42.8</td><td>53.2</td><td>49.7</td><td>37.5</td><td>62.5</td><td>39.8</td><td>47.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Alibaba</td><td>Qwen2-VL-72B-AWQ</td><td>2024-09</td><td>43.0</td><td>46.2</td><td>45.2</td><td>43.8</td><td>75.0</td><td>46.6</td><td>45.5</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>DAMO</td><td>VideoLLama3-7B</td><td>2025-01</td><td>47.4</td><td>45.0</td><td>45.8</td><td>39.5</td><td>60.0</td><td>41.4</td><td>44.7</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Alibaba</td><td>Qwen2.5-VL-7B</td><td>2025-01</td><td>42.3</td><td>45.0</td><td>44.1</td><td>39.3</td><td>55.0</td><td>40.7</td><td>43.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>DAMO</td><td>VideoLLama3-2B</td><td>2025-01</td><td>48.6</td><td>43.7</td><td>45.3</td><td>29.0</td><td>60.0</td><td>31.8</td><td>42.2</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Rhymes</td><td>Aria</td><td>2024-11</td><td>42.3</td><td>44.0</td><td>43.5</td><td>35.3</td><td>57.5</td><td>37.3</td><td>42.0</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Shanghai&nbsp;AI&nbsp;Lab</td><td>InternVL2.5-8B</td><td>2024-11</td><td>40.8</td><td>41.1</td><td>41.0</td><td>40.8</td><td>55.0</td><td>42.1</td><td>41.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Meta</td><td>Llama-3.2-90B-Vision</td><td>2024-09</td><td>37.4</td><td>42.4</td><td>40.8</td><td>28.0</td><td>85.0</td><td>33.2</td><td>38.9</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Alibaba</td><td>Qwen2-VL-7B</td><td>2024-08</td><td>36.1</td><td>38.2</td><td>37.5</td><td>38.5</td><td>37.5</td><td>38.4</td><td>37.7</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>OpenGVLab</td><td>InternVideo2-8B</td><td>2024-08</td><td>37.2</td><td>37.9</td><td>37.6</td><td>40.5</td><td>0.0</td><td>36.8</td><td>37.4</td></tr>

    <!-- 21 ‚Äì 34 -->
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Meta</td><td>Llama-3.2-11B-Vision</td><td>2024-09</td><td>35.2</td><td>36.1</td><td>35.8</td><td>38.3</td><td>62.5</td><td>40.5</td><td>36.9</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Shanghai&nbsp;AI&nbsp;Lab</td><td>InternVL2-8B</td><td>2024-06</td><td>33.2</td><td>38.2</td><td>36.5</td><td>34.8</td><td>72.5</td><td>38.2</td><td>36.9</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>DAMO</td><td>VideoLLama2.1-7B</td><td>2024-10</td><td>43.0</td><td>36.0</td><td>38.2</td><td>31.5</td><td>40.0</td><td>32.3</td><td>36.8</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Microsoft</td><td>Phi-4-Multimodal</td><td>2025-03</td><td>39.9</td><td>36.0</td><td>37.3</td><td>34.8</td><td>2.5</td><td>31.8</td><td>36.0</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Microsoft</td><td>Phi-3.5-Vision</td><td>2024-07</td><td>36.3</td><td>39.1</td><td>38.2</td><td>26.5</td><td>42.5</td><td>28.0</td><td>35.7</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>HuggingFaceM4</td><td>Idefics3-8B</td><td>2024-08</td><td>34.3</td><td>36.2</td><td>35.6</td><td>33.5</td><td>42.5</td><td>34.3</td><td>35.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>LLaVA</td><td>LLaVA-NeXT-Video-34B</td><td>2024-06</td><td>37.2</td><td>34.9</td><td>35.7</td><td>31.5</td><td>60.0</td><td>34.1</td><td>35.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Mistral&nbsp;AI</td><td>Pixtral-12B</td><td>2024-09</td><td>36.3</td><td>32.9</td><td>34.0</td><td>41.0</td><td>17.5</td><td>38.9</td><td>35.2</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>DeepSeek</td><td>DeepSeek-VL2-Tiny</td><td>2024-12</td><td>31.4</td><td>32.5</td><td>32.2</td><td>42.8</td><td>15.0</td><td>40.2</td><td>34.1</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>LLaVA</td><td>LLaVA-One-Vision-7B</td><td>2024-09</td><td>32.5</td><td>33.1</td><td>32.9</td><td>32.8</td><td>45.0</td><td>33.9</td><td>33.1</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>H2O</td><td>H2OVL-Mississippi-2B</td><td>2024-10</td><td>37.0</td><td>33.3</td><td>34.5</td><td>27.3</td><td>27.5</td><td>27.3</td><td>32.7</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>LLaVA</td><td>LLaVA-NeXT-Video-7B</td><td>2024-06</td><td>30.3</td><td>30.9</td><td>30.7</td><td>24.5</td><td>25.0</td><td>24.6</td><td>29.2</td></tr>
    
    <tr style="background-color:rgba(200,200,255,0.3);"><td>DAMO</td><td>VideoLLama2-7B</td><td>2024-06</td><td>36.3</td><td>16.5</td><td>23.0</td><td>25.8</td><td>37.5</td><td>26.8</td><td>23.9</td></tr>
  </tbody>

</table>

          <div class="model-labels-container">
            <span class="leaderboard-label" style="background-color: rgba(200, 200, 255, 1); margin-right: 30px;">Open-Source</span>
            <span class="leaderboard-label" style="background-color: rgba(255, 200, 200, 1);">Proprietary</span>
          </div>
          <p> More models are coming. </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhou2025vlm4d,
    title={VLM4D: Towards Spatiotemporal Awareness in Vision Language Models},
    author={Zhou, Shijie and Vilesov, Alexander and He, Xuehai and Wan, Ziyu and Zhang, Shuwang and Nagachandra, Aditya and Chang, Di and Chen, Dongdong and Wang, Eric Xin and Kadambi, Achuta},
    year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a rel="license"
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a rel="license"
            href="https://gligen.github.io/">GLIGEN</a>, licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
