<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VLM4D: Towards Spatiotemporal Awareness in Vision Language Models">
  <meta name="keywords" content="VLM, spatiotemporal reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="PvilQQmOfgqfpL57j2H43EAQ1-rlZQdgZ7Wrp6cnIYk" />
  <title>VLM4D: Towards Spatiotemporal Awareness in Vision Language Models</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">
</head>
<body>



  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon.png">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- <script src="https://assets.crowd.aws/crowd-html-elements.js"></script> -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

<style>
/* Demo section custom styles */
.demo-option {
  min-width: 150px;
  transition: all 0.3s ease;
}

.demo-option:hover {
  transform: translateY(-2px);
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
}

#demo-video {
  border-radius: 10px;
  box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
}

#demo .box {
  border-radius: 15px;
  box-shadow: 0 6px 20px rgba(0, 0, 0, 0.1);
}

/* Tab styles */
.tabs.is-boxed li.is-active a {
  background-color: #3273dc;
  border-color: #3273dc;
  color: white;
}

.tabs.is-boxed li a {
  border: 1px solid #dbdbdb;
  border-radius: 4px 4px 0 0;
}

.tabs.is-boxed li a:hover {
  background-color: #f5f5f5;
  border-bottom-color: #dbdbdb;
}

/* Progress bar container */
.progress {
  width: 200px;
}

/* Button animations */
.demo-option.correct {
  animation: correctAnswer 0.6s ease;
}

.demo-option.incorrect {
  animation: incorrectAnswer 0.6s ease;
}

@keyframes correctAnswer {
  0% { transform: scale(1); }
  50% { transform: scale(1.05); }
  100% { transform: scale(1); }
}

@keyframes incorrectAnswer {
  0% { transform: translateX(0); }
  25% { transform: translateX(-5px); }
  75% { transform: translateX(5px); }
  100% { transform: translateX(0); }
}
</style>



</section>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VLM4D: Towards Spatiotemporal Awareness in Vision Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ShijieZhou-UCLA.github.io/">Shijie Zhou</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://asvilesov.github.io/">Alexander Vilesov</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://sheehan1230.github.io/">Xuehai He</a><sup>2,3*</sup>,</span>
            <span class="author-block">
              <a href="http://raywzy.com/">Ziyu Wan</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href=" ">Shuwang Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://adityanagachandra.github.io/">Aditya Nagachandra</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href=" ">Di Chang</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://www.dongdongchen.bid/">Dongdong Chen</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://eric-xw.github.io/">Xin Eric Wang</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://samueli.ucla.edu/people/achuta-kadambi/">Achuta Kadambi</a><sup>1</sup>,</span>
          </div>
          
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">* Equal Contribution</span>
          </div> -->
          <style>
            .univerity-block {
                margin-right: 14px;
            }
          </style>
          <div class="is-size-5 publication-authors">
            <span class="univerity-block"><sup>1</sup>UCLA</span>
            <span class="univerity-block"><sup>2</sup>Microsoft</span>
            <span class="univerity-block"><sup>3</sup>UCSC</span>
            <span class="univerity-block"><sup>4</sup>USC</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block">*Equal Contribution</span>
          </div>

          <div class="is-size-5 publication-authors" style="margin-top: 0.5rem;">
            <span class="univerity-block" style="color: rgb(169, 16, 151);">ICCV 2025, Honolulu</span>
          </div>

          <div class="publication-links" style="margin-top: 2rem;">
              <!-- PDF Link. -->

              <span class="link-block">
                <a href="https://www.arxiv.org/pdf/2508.02095"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/shijiezhou/VLM4D" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

              <span class="link-block">
                <a href="#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon has-text-white">
                    <i class="fa-solid fa-trophy"></i>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Demo Section -->
<section class="section" id="demo">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">
          <img id="demo_icon" width="5%" src="static/images/icon/question_rny.png"> 
          Interactive Demo
        </h2> 
        <p class="subtitle is-5">Test your spatiotemporal reasoning skills with our VLM4D benchmark questions</p>
      </div>
    </div>

    <!-- Category Selection -->
    <div class="columns is-centered has-text-centered" style="margin-bottom: 2rem;">
      <div class="column is-four-fifths">
        <div class="tabs is-centered is-large is-boxed">
          <ul>
            <li class="tab-link is-active" data-category="ego-centric">
              <a><strong>Ego-centric</strong></a>
            </li>
            <li class="tab-link" data-category="exo-centric">
              <a><strong>Exo-centric</strong></a>
            </li>
            <li class="tab-link" data-category="synthetic">
              <a><strong>Synthetic</strong></a>
            </li>
          </ul>
        </div>
      </div>
    </div>

    <!-- Demo Content -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <div class="box">
          <!-- Question Navigation -->
          <div class="level" style="margin-bottom: 1rem;">
            <div class="level-left">
              <div class="level-item">
                <span class="tag is-info is-medium">
                  <span id="current-question-info">Question 1 of 3</span>
                </span>
              </div>
            </div>
            <div class="level-right">
              <div class="level-item">
                <div class="buttons">
                  <button id="prev-question" class="button is-small" disabled>
                    <span class="icon"><i class="fas fa-chevron-left"></i></span>
                    <span>Previous</span>
                  </button>
                  <button id="next-question" class="button is-small">
                    <span>Next</span>
                    <span class="icon"><i class="fas fa-chevron-right"></i></span>
                  </button>
                </div>
              </div>
            </div>
          </div>

          <!-- Video Player -->
          <video id="demo-video" class="is-fullwidth" controls preload="metadata" style="max-height: 400px;">
            <source src="https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/parkour.mp4" type="video/mp4">
            <source src="file/parkour.mp4" type="video/mp4">
            <source src="./file/parkour.mp4" type="video/mp4">
            <p>Your browser does not support the video tag. Please try opening the video directly: <a href="https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/parkour.mp4">parkour.mp4</a></p>
          </video>
          
          <!-- Question Content -->
          <div style="margin-top: 2rem;">
            <h3 id="question-text" class="title is-4" style="color: #363636;">Which direction is he running toward?</h3>
            
            <div class="buttons is-centered" style="margin-top: 1.5rem;" id="answer-buttons">
              <!-- Answer buttons will be dynamically generated -->
            </div>
            
            <!-- Feedback -->
            <div id="demo-feedback" style="margin-top: 1rem; display: none;">
              <div class="notification" id="feedback-message"></div>
            </div>

            <!-- Progress Indicator -->
            <div style="margin-top: 2rem;">
              <div class="columns is-multiline is-centered">
                <div class="column is-narrow">
                  <span class="tag is-light">Score: <span id="score-display">0/0</span></span>
                </div>
                <div class="column is-narrow">
                  <progress id="progress-bar" class="progress is-primary" value="0" max="100">0%</progress>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="7%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Spatiotemporal (4D) Awareness. </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">  
            <img id="model" width="80%" src="./static/images/figures/teaster_v5.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 1. Spatiotemporal (4D) Awareness. Humans intuitively reason in 4D (3D space + time), effortlessly reconstructing the dynamic spatial trajectory of moving objects from any perspective. In contrast, current Vision Language Models (VLMs) typically rely on aggregating 2D visual features across time, leading to incorrect predictions when motion understanding and interpretation requires deeper spatiotemporal reasoning. In this example, humans correctly perceive the car moving to the right, while the VLM (GPT-4o) inaccurately predicts leftward movement, suggesting VLMs struggle to perform spatiotemporal reasoning. </b></p>
            </h3>   
        </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-language models (VLMs) have shown remarkable capabilities in integrating linguistic and visual reasoning but remain fundamentally limited in understanding dynamic spatiotemporal interactions. Humans effortlessly track and reason about object movements, rotations, and perspective shifts—abilities essential for robust real-world understanding yet notably lacking in current VLMs. In this paper, we introduce VLM4D, the first benchmark specifically designed to evaluate the spatiotemporal reasoning capabilities of VLMs. Our benchmark comprises diverse real-world and synthetic videos accompanied by carefully curated question-answer pairs emphasizing translational and rotational motions, perspective awareness, and motion continuity. Through comprehensive evaluations of state-of-the-art open and closed-source VLMs, we identify significant performance gaps compared to human baselines, highlighting fundamental deficiencies in existing models. Extensive analysis reveals that VLMs struggle particularly with integrating multiple visual cues and maintaining temporal coherence. We further explore promising directions, such as leveraging 4D feature field reconstruction and targeted spatiotemporal supervised fine-tuning, demonstrating their effectiveness in enhancing spatiotemporal comprehension. Our work aims to encourage deeper exploration into improving VLMs’ spatial and temporal grounding, paving the way towards more capable and reliable visual intelligence for dynamic environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Distribution of Dataset Sources and Annotations. </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">  
            <img id="model" width="30%" src="./static/images/figures/piechart_dataset_stats.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 2. Distribution of Dataset Sources and Annotations. Breakdown of our dataset illustrating the proportions of data sourced from third-person (Davis, YouTube), first-person (Ego4D), and synthetic data, categorized by annotation types: translational, rotational, action, counting, and false positives. </b></p>
            </h3>   
        </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Dataset Generation and Annotation</h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">  
            <img id="model" width="100%" src="./static/images/figures/dataset_generation_v2.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 3. Dataset Generation and Annotation Pipeline. Our dataset was constructed by collecting real videos and generating synthetic data, followed by human-in-the-loop quality reviews to address ambiguous videos and annotations. After temporal alignment and quality assurance, human-annotated questions and answers were created, complemented by multiple-choice questions generated by large language models (LLMs). The final dataset includes real-world and synthetic video data with comprehensive VLM scoring metrics. </b></p>
            </h3>   
        </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Qualitative Examples of Dataset Annotations. </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">     
            <img id="model" width="50%" src="./static/images/figures/dataset_qualitative_v3.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 4. (Top) A third-person video with translational annotations ("camel turning left from its perspective"). (Middle) A first-person video with a rotational question ("clockwise rotation of ladle"). (Bottom) A synthetic scene with action recognition "robotic dog moving left".</b></p>
            </h3>   


        </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Model Performance</h2> 
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-half">  
        <img id="model" width="100%" src="./static/images/figures/radar_plot.png">
      </div>
      <div class="column is-half">  
        <img id="model" width="100%" src="./static/images/figures/cot_vs_do.png">
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"><b>Figure 5. Model Accuracy Across Real Scene Question Categories of top-performing VLMs (left). Comparison of CoT and DO Accuracy Across Models (right).</b></p>
        </h3>   
      </div>
    </div>
  </div>
</section>






<section class="section" id="leaderboard">
  <div class="container is-max-desktop">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Leaderboard</h2>
        <div class="content">
          <div class="content has-text-justified">
            <p>
              Evaluation on VLM4D Benchmark across various proprietary and open-source VLMs.
            </p>
          </div>    
          <!-- Validation Set Leaderboard -->
          <!---- Updated scores sourced from the June 2025 LaTeX results -->
<table id="table1" class="js-sort-table">
  <!-- <thead>
    <tr>
      <th class="js-sort-string"><strong>Organization</strong></th>
      <th class="js-sort-string"><strong>Model</strong></th>
      <th class="js-sort-string"><strong>Release</strong></th>
      <th class="js-sort-number"><strong>Ego-centric</strong></th>
      <th class="js-sort-number"><strong>Exo-centric</strong></th>
      <th class="js-sort-number"><strong>Real&nbsp;Avg</strong></th>
      <th class="js-sort-number"><strong>Directional</strong></th>
      <th class="js-sort-number"><strong>FP</strong></th>
      <th class="js-sort-number"><strong>Synth&nbsp;Avg</strong></th>
      <th class="js-sort-number"><strong>Overall</strong></th>
    </tr>
  </thead> -->

    <!-- <tbody>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>User&nbsp;Study</td><td>Human&nbsp;Performance</td><td></td><td>99.6</td><td>99.7</td><td>99.7</td><td>91.8</td><td>100.0</td><td>95.9</td><td>98.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Random</td><td>Random&nbsp;Selection</td><td></td><td>24.4</td><td>23.2</td><td>23.6</td><td>25.5</td><td>24.7</td><td>25.1</td><td>24.2</td></tr>
    <tr style="background-color:rgba(255,200,200,0.3);"><td>Google</td><td>Gemini-2.5-Pro</td><td>2025-03</td><td>68.2</td><td>70.5</td><td>69.7</td><td>71.3</td><td>75.0</td><td>71.6</td><td>70.2</td></tr>
    <tr style="background-color:rgba(255,200,200,0.3);"><td>Anthropic</td><td>Claude-3.7-Sonnet</td><td>2025-02</td><td>51.2</td><td>65.0</td><td>60.5</td><td>45.3</td><td>93.3</td><td>50.1</td><td>57.9</td></tr>
    <tr style="background-color:rgba(255,200,200,0.3);"><td>OpenAI</td><td>GPT-4o</td><td>2024-11</td><td>54.3</td><td>61.2</td><td>58.9</td><td>47.8</td><td>47.5</td><td>47.7</td><td>56.2</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Meta</td><td>Llama-4-Maverick-17B</td><td>2025-04</td><td>48.8</td><td>52.9</td><td>51.6</td><td>56.0</td><td>50.0</td><td>55.5</td><td>52.5</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Alibaba</td><td>Qwen2.5-VL-72B-AWQ</td><td>2025-01</td><td>49.9</td><td>48.7</td><td>49.1</td><td>54.3</td><td>75.0</td><td>56.1</td><td>50.8</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>OpenGVLab</td><td>InternVideo2.5-8B</td><td>2025-01</td><td>52.8</td><td>50.1</td><td>51.0</td><td>45.3</td><td>32.5</td><td>44.1</td><td>49.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Meta</td><td>Llama-4-Scout-17B</td><td>2025-04</td><td>46.6</td><td>51.3</td><td>49.7</td><td>46.8</td><td>45.0</td><td>46.6</td><td>49.0</td></tr>
    <tr style="background-color:rgba(255,200,200,0.3);"><td>xAI</td><td>Grok-2-Vision</td><td>2024-12</td><td>44.1</td><td>48.8</td><td>47.3</td><td>49.0</td><td>75.0</td><td>51.4</td><td>48.3</td></tr>
    <tr style="background-color:rgba(255,200,200,0.3);"><td>Google</td><td>Gemini-2.0-Pro</td><td>2025-02</td><td>44.8</td><td>50.5</td><td>48.7</td><td>42.8</td><td>52.5</td><td>43.6</td><td>47.4</td></tr>

    <tr style="background-color:rgba(200,200,255,0.3);"><td>Shanghai&nbsp;AI&nbsp;Lab</td><td>InternVL2.5-38B</td><td>2024-11</td><td>42.8</td><td>53.2</td><td>49.7</td><td>37.5</td><td>62.5</td><td>39.8</td><td>47.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Alibaba</td><td>Qwen2-VL-72B-AWQ</td><td>2024-09</td><td>43.0</td><td>46.2</td><td>45.2</td><td>43.8</td><td>75.0</td><td>46.6</td><td>45.5</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>DAMO</td><td>VideoLLama3-7B</td><td>2025-01</td><td>47.4</td><td>45.0</td><td>45.8</td><td>39.5</td><td>60.0</td><td>41.4</td><td>44.7</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Alibaba</td><td>Qwen2.5-VL-7B</td><td>2025-01</td><td>42.3</td><td>45.0</td><td>44.1</td><td>39.3</td><td>55.0</td><td>40.7</td><td>43.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>DAMO</td><td>VideoLLama3-2B</td><td>2025-01</td><td>48.6</td><td>43.7</td><td>45.3</td><td>29.0</td><td>60.0</td><td>31.8</td><td>42.2</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Rhymes</td><td>Aria</td><td>2024-11</td><td>42.3</td><td>44.0</td><td>43.5</td><td>35.3</td><td>57.5</td><td>37.3</td><td>42.0</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Shanghai&nbsp;AI&nbsp;Lab</td><td>InternVL2.5-8B</td><td>2024-11</td><td>40.8</td><td>41.1</td><td>41.0</td><td>40.8</td><td>55.0</td><td>42.1</td><td>41.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Meta</td><td>Llama-3.2-90B-Vision</td><td>2024-09</td><td>37.4</td><td>42.4</td><td>40.8</td><td>28.0</td><td>85.0</td><td>33.2</td><td>38.9</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Alibaba</td><td>Qwen2-VL-7B</td><td>2024-08</td><td>36.1</td><td>38.2</td><td>37.5</td><td>38.5</td><td>37.5</td><td>38.4</td><td>37.7</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>OpenGVLab</td><td>InternVideo2-8B</td><td>2024-08</td><td>37.2</td><td>37.9</td><td>37.6</td><td>40.5</td><td>0.0</td><td>36.8</td><td>37.4</td></tr>

    <tr style="background-color:rgba(200,200,255,0.3);"><td>Meta</td><td>Llama-3.2-11B-Vision</td><td>2024-09</td><td>35.2</td><td>36.1</td><td>35.8</td><td>38.3</td><td>62.5</td><td>40.5</td><td>36.9</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Shanghai&nbsp;AI&nbsp;Lab</td><td>InternVL2-8B</td><td>2024-06</td><td>33.2</td><td>38.2</td><td>36.5</td><td>34.8</td><td>72.5</td><td>38.2</td><td>36.9</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>DAMO</td><td>VideoLLama2.1-7B</td><td>2024-10</td><td>43.0</td><td>36.0</td><td>38.2</td><td>31.5</td><td>40.0</td><td>32.3</td><td>36.8</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Microsoft</td><td>Phi-4-Multimodal</td><td>2025-03</td><td>39.9</td><td>36.0</td><td>37.3</td><td>34.8</td><td>2.5</td><td>31.8</td><td>36.0</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Microsoft</td><td>Phi-3.5-Vision</td><td>2024-07</td><td>36.3</td><td>39.1</td><td>38.2</td><td>26.5</td><td>42.5</td><td>28.0</td><td>35.7</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>HuggingFaceM4</td><td>Idefics3-8B</td><td>2024-08</td><td>34.3</td><td>36.2</td><td>35.6</td><td>33.5</td><td>42.5</td><td>34.3</td><td>35.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>LLaVA</td><td>LLaVA-NeXT-Video-34B</td><td>2024-06</td><td>37.2</td><td>34.9</td><td>35.7</td><td>31.5</td><td>60.0</td><td>34.1</td><td>35.3</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>Mistral&nbsp;AI</td><td>Pixtral-12B</td><td>2024-09</td><td>36.3</td><td>32.9</td><td>34.0</td><td>41.0</td><td>17.5</td><td>38.9</td><td>35.2</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>DeepSeek</td><td>DeepSeek-VL2-Tiny</td><td>2024-12</td><td>31.4</td><td>32.5</td><td>32.2</td><td>42.8</td><td>15.0</td><td>40.2</td><td>34.1</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>LLaVA</td><td>LLaVA-One-Vision-7B</td><td>2024-09</td><td>32.5</td><td>33.1</td><td>32.9</td><td>32.8</td><td>45.0</td><td>33.9</td><td>33.1</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>H2O</td><td>H2OVL-Mississippi-2B</td><td>2024-10</td><td>37.0</td><td>33.3</td><td>34.5</td><td>27.3</td><td>27.5</td><td>27.3</td><td>32.7</td></tr>
    <tr style="background-color:rgba(200,200,255,0.3);"><td>LLaVA</td><td>LLaVA-NeXT-Video-7B</td><td>2024-06</td><td>30.3</td><td>30.9</td><td>30.7</td><td>24.5</td><td>25.0</td><td>24.6</td><td>29.2</td></tr>
    
    <tr style="background-color:rgba(200,200,255,0.3);"><td>DAMO</td><td>VideoLLama2-7B</td><td>2024-06</td><td>36.3</td><td>16.5</td><td>23.0</td><td>25.8</td><td>37.5</td><td>26.8</td><td>23.9</td></tr>
  </tbody> -->
  <tbody>
    <tr style="border-bottom: 1px solid black;">
        <td rowspan="2" style="font-weight:bold; text-align:left; vertical-align:middle; border-bottom: 2px solid black;">Organization</td>
        <td rowspan="2" style="font-weight:bold; text-align:left; vertical-align:middle; border-bottom: 2px solid black;">Model</td>
        <td rowspan="2" style="font-weight:bold; text-align:center; vertical-align:middle; border-bottom: 2px solid black;">Release</td>
        <td colspan="3" style="font-weight:bold; text-align:center;">Real</td>
        <td colspan="3" style="font-weight:bold; text-align:center;">Synthetic</td>
        <td rowspan="2" style="font-weight:bold; text-align:center; vertical-align:middle; border-bottom: 2px solid black;">Overall</td>
    </tr>
    <tr>
        <td style="font-weight:bold; text-align:center; border-bottom: 2px solid black;">Ego-centric</td>
        <td style="font-weight:bold; text-align:center; border-bottom: 2px solid black;">Exo-centric</td>
        <td style="font-weight:bold; text-align:center; border-bottom: 2px solid black;">Average</td>
        <td style="font-weight:bold; text-align:center; border-bottom: 2px solid black;">Directional</td>
        <td style="font-weight:bold; text-align:center; border-bottom: 2px solid black;">FP</td>
        <td style="font-weight:bold; text-align:center; border-bottom: 2px solid black;">Average</td>
    </tr>

    <tr style="border-top: 2px solid black;">
        <td>User Study</td>
        <td>Human Performance</td>
        <td></td>
        <td>99.6</td>
        <td>99.7</td>
        <td>99.7</td>
        <td>95.8</td>
        <td>100</td>
        <td>96.2</td>
        <td>98.8</td>
    </tr>
    <tr>
        <td>Random</td>
        <td>Random Selection</td>
        <td></td>
        <td>24.4</td>
        <td>23.2</td>
        <td>23.6</td>
        <td>25.5</td>
        <td>24.7</td>
        <td>25.4</td>
        <td>24.1</td>
    </tr>

    <tr style="border-top: 2px solid black;">
        <td colspan="10" style="background-color:#e1f0f5; font-weight:bold; text-align:left;">Latest Proprietary VLMs</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>OpenAI</td>
        <td>GPT-4o</td>
        <td>2024-11</td>
        <td style="background-color:#FFE5CC;">55.5</td>
        <td style="background-color:#FFCC99;">62.2</td>
        <td style="background-color:#FFCC99;">60.0</td>
        <td style="background-color:#FFE5CC;">49.5</td>
        <td>53.3</td>
        <td>49.9</td>
        <td style="background-color:#FFCC99;">57.5</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>Google</td>
        <td>Gemini-2.5-Pro</td>
        <td>2025-6</td>
        <td style="background-color:#FFB366;">64.6</td>
        <td style="background-color:#FFB366;">62.9</td>
        <td style="background-color:#FFB366;">63.5</td>
        <td style="background-color:#FFB366;">54.8</td>
        <td style="background-color:#FFCC99;">80.0</td>
        <td style="background-color:#FFB366;">57.3</td>
        <td style="background-color:#FFB366;">62.0</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>Anthropic</td>
        <td>Claude-Sonnet-4</td>
        <td>2025-5</td>
        <td>52.6</td>
        <td>52.1</td>
        <td>52.2</td>
        <td>44.0</td>
        <td style="background-color:#FFB366;">86.7</td>
        <td>48.3</td>
        <td>51.3</td>
    </tr>
    <tr>
        <td>xAI</td>
        <td>Grok-2-Vision</td>
        <td>2024-12</td>
        <td>48.8</td>
        <td>49.7</td>
        <td>49.4</td>
        <td>49.3</td>
        <td>66.7</td>
        <td>51.0</td>
        <td>49.8</td>
    </tr>

    <tr style="border-top: 2px solid black;">
        <td colspan="10" style="background-color:#e1f0f5; font-weight:bold; text-align:left;">Open-source Image VLMs</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td rowspan="2" style="vertical-align:middle;">Meta</td>
        <td>Llama-4-Maverick-17B</td>
        <td>2025-4</td>
        <td>52.6</td>
        <td>54.3</td>
        <td style="background-color:#FFE5CC;">53.8</td>
        <td style="background-color:#FFCC99;">53.3</td>
        <td>51.1</td>
        <td style="background-color:#FFE5CC;">53.0</td>
        <td>53.6</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>Llama-4-Scout-17B</td>
        <td>2025-4</td>
        <td>48.6</td>
        <td style="background-color:#FFE5CC;">56.2</td>
        <td>53.7</td>
        <td style="background-color:#FFCC99;">53.3</td>
        <td style="background-color:#FFE5CC;">75.6</td>
        <td style="background-color:#FFCC99;">55.5</td>
        <td style="background-color:#FFE5CC;">54.1</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td rowspan="2" style="vertical-align:middle;">Microsoft</td>
        <td>Phi-4-Multimodal</td>
        <td>2025-3</td>
        <td>41.0</td>
        <td>35.4</td>
        <td>37.2</td>
        <td>37.5</td>
        <td>11.1</td>
        <td>34.8</td>
        <td>36.6</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>Phi-3.5-Vision</td>
        <td>2024-7</td>
        <td>33.4</td>
        <td>38.8</td>
        <td>37.1</td>
        <td>23.3</td>
        <td>37.8</td>
        <td>24.7</td>
        <td>34.0</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>DeepSeek</td>
        <td>DeepSeek-VL2</td>
        <td>2024-12</td>
        <td>33.6</td>
        <td>32.9</td>
        <td>33.1</td>
        <td>31.8</td>
        <td>46.7</td>
        <td>33.3</td>
        <td>33.2</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td rowspan="2" style="vertical-align:middle;">Shanghai AI Lab</td>
        <td>InternVL2.5-38B</td>
        <td>2024-11</td>
        <td>46.6</td>
        <td>50.1</td>
        <td>48.9</td>
        <td>43.3</td>
        <td>57.8</td>
        <td>44.7</td>
        <td>47.9</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>InternVL2.5-8B</td>
        <td>2024-11</td>
        <td>39.0</td>
        <td>44.0</td>
        <td>42.4</td>
        <td>40.8</td>
        <td>42.2</td>
        <td>40.9</td>
        <td>42.0</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>Mistral AI</td>
        <td>Pixtral-12B</td>
        <td>2024-9</td>
        <td>32.3</td>
        <td>25.8</td>
        <td>27.9</td>
        <td>24.3</td>
        <td>22.2</td>
        <td>24.0</td>
        <td>27.0</td>
    </tr>
    <tr>
        <td>Rhymes</td>
        <td>Aria</td>
        <td>2024-11</td>
        <td>47.2</td>
        <td>44.0</td>
        <td>45.1</td>
        <td>38.5</td>
        <td>71.1</td>
        <td>41.8</td>
        <td>44.3</td>
    </tr>

    <tr style="border-top: 2px solid black;">
        <td colspan="10" style="background-color:#e1f0f5; font-weight:bold; text-align:left;">Open-source Video VLMs</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td rowspan="4" style="vertical-align:middle;">Alibaba</td>
        <td>Qwen2.5-VL-7B</td>
        <td>2025-1</td>
        <td>42.3</td>
        <td>43.7</td>
        <td>43.3</td>
        <td>43.5</td>
        <td>64.4</td>
        <td>45.6</td>
        <td>43.8</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>Qwen2.5-VL-72B</td>
        <td>2025-1</td>
        <td>54.3</td>
        <td>52.5</td>
        <td>53.1</td>
        <td style="background-color:#FFE5CC;">49.5</td>
        <td style="background-color:#FFCC99;">80.0</td>
        <td>52.6</td>
        <td>53.0</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>Qwen2-VL-7B</td>
        <td>2024-8</td>
        <td>36.1</td>
        <td>34.7</td>
        <td>35.2</td>
        <td>40.5</td>
        <td>35.6</td>
        <td>40.0</td>
        <td>36.3</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>Qwen2-VL-72B</td>
        <td>2024-9</td>
        <td>48.1</td>
        <td>43.0</td>
        <td>44.6</td>
        <td>40.8</td>
        <td>73.3</td>
        <td>44.0</td>
        <td>44.5</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td rowspan="2" style="vertical-align:middle;">DAMO</td>
        <td>VideoLLama3-2B</td>
        <td>2025-1</td>
        <td>53.2</td>
        <td>42.5</td>
        <td>46.0</td>
        <td>34.3</td>
        <td>55.6</td>
        <td>36.4</td>
        <td>43.7</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>VideoLLama3-7B</td>
        <td>2025-1</td>
        <td>49.4</td>
        <td>45.1</td>
        <td>46.5</td>
        <td>42.8</td>
        <td>53.3</td>
        <td>43.8</td>
        <td>45.9</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td rowspan="2" style="vertical-align:middle;">Shanghai AI Lab</td>
        <td>InternVideo2.5-8B</td>
        <td>2025-1</td>
        <td style="background-color:#FFCC99;">57.2</td>
        <td>50.5</td>
        <td>52.7</td>
        <td>44.3</td>
        <td>46.7</td>
        <td>44.5</td>
        <td>50.7</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td>InternVideo2-8B</td>
        <td>2024-8</td>
        <td>35.6</td>
        <td>39.3</td>
        <td>38.1</td>
        <td>43.0</td>
        <td>0.0</td>
        <td>38.7</td>
        <td>38.2</td>
    </tr>
    <tr style="border-bottom: 1px dashed lightgray;">
        <td rowspan="2" style="vertical-align:middle;">LLaVA</td>
        <td>LLaVA-One-Vision-7B</td>
        <td>2024-9</td>
        <td>36.8</td>
        <td>35.6</td>
        <td>36.0</td>
        <td>37.8</td>
        <td>35.6</td>
        <td>37.5</td>
        <td>36.3</td>
    </tr>
    <tr>
        <td>LLaVA-NeXT-Video-34B</td>
        <td>2024-6</td>
        <td>29.6</td>
        <td>31.6</td>
        <td>30.9</td>
        <td>24.5</td>
        <td>55.6</td>
        <td>27.6</td>
        <td>30.1</td>
    </tr>
</tbody>

</table>

          <!-- <div class="model-labels-container">
            <span class="leaderboard-label" style="background-color: rgba(200, 200, 255, 1); margin-right: 30px;">Open-Source</span>
            <span class="leaderboard-label" style="background-color: rgba(255, 200, 200, 1);">Proprietary</span>
          </div>
          <p> More models are coming. </p> -->
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{zhou2025vlm4d,
    title={VLM4D: Towards Spatiotemporal Awareness in Vision Language Models},
    author={Zhou, Shijie and Vilesov, Alexander and He, Xuehai and Wan, Ziyu and Zhang, Shuwang and Nagachandra, Aditya and Chang, Di and Chen, Dongdong and Wang, Eric Xin and Kadambi, Achuta},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a rel="license"
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a rel="license"
            href="https://gligen.github.io/">GLIGEN</a>, licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
// Demo interaction script with multiple questions
document.addEventListener('DOMContentLoaded', function() {
    // Question database
    const questionDatabase = {
        'ego-centric': [
            {
                video: 'https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/parkour.mp4',
                question: 'From the first-person perspective, which direction am I moving?',
                options: ['A: Forward', 'B: Backward', 'C: Left', 'D: Right'],
                correct: 'A',
                explanation: 'In first-person view, the forward movement is clearly visible through the camera motion.'
            },
            {
                video: 'https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/parkour.mp4',
                question: 'In this ego-centric view, how is the camera rotating?',
                options: ['A: Clockwise', 'B: Counter-clockwise', 'C: No rotation', 'D: Up and down'],
                correct: 'B',
                explanation: 'The ego-centric perspective shows a counter-clockwise rotation as the viewpoint shifts.'
            },
            {
                video: 'https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/parkour.mp4',
                question: 'From my perspective, what is the primary motion direction?',
                options: ['A: Upward', 'B: Downward', 'C: Leftward', 'D: Rightward'],
                correct: 'C',
                explanation: 'The first-person view shows movement primarily to the left direction.'
            }
        ],
        'exo-centric': [
            {
                video: 'https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/parkour.mp4',
                question: 'From an external viewpoint, which direction is the person running?',
                options: ['A: Left', 'B: Right', 'C: Toward camera', 'D: Away from camera'],
                correct: 'A',
                explanation: 'From the third-person perspective, the person is clearly moving to the left.'
            },
            {
                video: 'https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/parkour.mp4',
                question: 'In this exo-centric view, how is the subject oriented relative to the camera?',
                options: ['A: Facing camera', 'B: Back to camera', 'C: Side profile', 'D: Upside down'],
                correct: 'C',
                explanation: 'The external viewpoint shows the subject in a side profile orientation.'
            },
            {
                video: 'https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/parkour.mp4',
                question: 'What type of movement pattern is visible in this third-person view?',
                options: ['A: Linear motion', 'B: Circular motion', 'C: Oscillating motion', 'D: Random motion'],
                correct: 'A',
                explanation: 'The third-person perspective clearly shows a linear movement pattern.'
            }
        ],
        'synthetic': [
            {
                video: 'https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/parkour.mp4',
                question: 'In this synthetic scene, which direction is the object moving?',
                options: ['A: North', 'B: South', 'C: East', 'D: West'],
                correct: 'C',
                explanation: 'The synthetic data shows clear eastward movement of the primary object.'
            },
            {
                video: 'https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/parkour.mp4',
                question: 'What is the trajectory pattern in this synthetic environment?',
                options: ['A: Straight line', 'B: Curved path', 'C: Zigzag pattern', 'D: Spiral motion'],
                correct: 'B',
                explanation: 'The synthetic scene demonstrates a curved trajectory pattern.'
            },
            {
                video: 'https://huggingface.co/datasets/shijiezhou/VLM4D/resolve/main/videos_real/davis/parkour.mp4',
                question: 'How many objects are in motion in this synthetic scene?',
                options: ['A: One', 'B: Two', 'C: Three', 'D: Four or more'],
                correct: 'A',
                explanation: 'Analysis shows only one primary object in motion in this synthetic scene.'
            }
        ]
    };

    // State management
    let currentCategory = 'ego-centric';
    let currentQuestionIndex = 0;
    let userAnswers = {};
    let totalScore = 0;

    // DOM elements
    const tabLinks = document.querySelectorAll('.tab-link');
    const videoElement = document.getElementById('demo-video');
    const questionText = document.getElementById('question-text');
    const answerButtonsContainer = document.getElementById('answer-buttons');
    const feedbackDiv = document.getElementById('demo-feedback');
    const feedbackMessage = document.getElementById('feedback-message');
    const prevButton = document.getElementById('prev-question');
    const nextButton = document.getElementById('next-question');
    const currentQuestionInfo = document.getElementById('current-question-info');
    const scoreDisplay = document.getElementById('score-display');
    const progressBar = document.getElementById('progress-bar');

    // Initialize
    function init() {
        initializeUserAnswers();
        setupTabListeners();
        setupNavigationListeners();
        loadQuestion();
        updateUI();
    }

    function initializeUserAnswers() {
        Object.keys(questionDatabase).forEach(category => {
            userAnswers[category] = new Array(questionDatabase[category].length).fill(null);
        });
    }

    function setupTabListeners() {
        tabLinks.forEach(tab => {
            tab.addEventListener('click', function(e) {
                e.preventDefault();
                const newCategory = this.getAttribute('data-category');
                if (newCategory !== currentCategory) {
                    switchCategory(newCategory);
                }
            });
        });
    }

    function setupNavigationListeners() {
        prevButton.addEventListener('click', function() {
            if (currentQuestionIndex > 0) {
                currentQuestionIndex--;
                loadQuestion();
                updateUI();
            }
        });

        nextButton.addEventListener('click', function() {
            const maxIndex = questionDatabase[currentCategory].length - 1;
            if (currentQuestionIndex < maxIndex) {
                currentQuestionIndex++;
                loadQuestion();
                updateUI();
            }
        });
    }

    function switchCategory(newCategory) {
        currentCategory = newCategory;
        currentQuestionIndex = 0;
        
        // Update tab appearance
        tabLinks.forEach(tab => {
            tab.classList.remove('is-active');
            if (tab.getAttribute('data-category') === newCategory) {
                tab.classList.add('is-active');
            }
        });

        loadQuestion();
        updateUI();
        hideFeedback();
    }

    function loadQuestion() {
        const currentQuestion = questionDatabase[currentCategory][currentQuestionIndex];
        
        // Update video source
        const sources = videoElement.querySelectorAll('source');
        sources.forEach(source => {
            source.src = currentQuestion.video;
        });
        videoElement.load();

        // Update question text
        questionText.textContent = currentQuestion.question;

        // Create answer buttons
        createAnswerButtons(currentQuestion.options, currentQuestion.correct);
    }

    function createAnswerButtons(options, correctAnswer) {
        answerButtonsContainer.innerHTML = '';
        
        options.forEach((option, index) => {
            const button = document.createElement('button');
            button.className = 'button is-medium demo-option';
            button.style.margin = '0.5rem';
            button.innerHTML = `<strong>${option}</strong>`;
            
            const optionLetter = option.charAt(0);
            button.setAttribute('data-answer', optionLetter);
            
            // Check if this question was already answered
            const userAnswer = userAnswers[currentCategory][currentQuestionIndex];
            if (userAnswer) {
                if (userAnswer === optionLetter) {
                    if (userAnswer === correctAnswer) {
                        button.classList.add('is-success');
                    } else {
                        button.classList.add('is-danger');
                    }
                }
                if (optionLetter === correctAnswer && userAnswer !== correctAnswer) {
                    button.classList.add('is-success');
                }
                button.disabled = true;
            }

            button.addEventListener('click', function() {
                handleAnswerSelection(optionLetter, correctAnswer);
            });

            answerButtonsContainer.appendChild(button);
        });
    }

    function handleAnswerSelection(selectedAnswer, correctAnswer) {
        const currentQuestion = questionDatabase[currentCategory][currentQuestionIndex];
        
        // Store user answer
        userAnswers[currentCategory][currentQuestionIndex] = selectedAnswer;

        // Update button states
        const buttons = answerButtonsContainer.querySelectorAll('.demo-option');
        buttons.forEach(button => {
            const buttonAnswer = button.getAttribute('data-answer');
            button.classList.remove('is-light');
            
            if (buttonAnswer === selectedAnswer) {
                if (selectedAnswer === correctAnswer) {
                    button.classList.add('is-success', 'correct');
                } else {
                    button.classList.add('is-danger', 'incorrect');
                }
            }
            
            if (buttonAnswer === correctAnswer && selectedAnswer !== correctAnswer) {
                button.classList.add('is-success');
            }
            
            button.disabled = true;
        });

        // Show feedback
        showFeedback(selectedAnswer === correctAnswer, currentQuestion.explanation);
        
        // Update score and progress
        updateScore();
        updateUI();
    }

    function showFeedback(isCorrect, explanation) {
        feedbackDiv.style.display = 'block';
        
        if (isCorrect) {
            feedbackMessage.className = 'notification is-success';
            feedbackMessage.innerHTML = `<strong>Correct!</strong> ${explanation}`;
        } else {
            feedbackMessage.className = 'notification is-warning';
            feedbackMessage.innerHTML = `<strong>Incorrect.</strong> ${explanation}`;
        }
    }

    function hideFeedback() {
        feedbackDiv.style.display = 'none';
    }

    function updateScore() {
        let correct = 0;
        let total = 0;
        
        Object.keys(userAnswers).forEach(category => {
            userAnswers[category].forEach((answer, index) => {
                if (answer !== null) {
                    total++;
                    if (answer === questionDatabase[category][index].correct) {
                        correct++;
                    }
                }
            });
        });
        
        totalScore = correct;
        scoreDisplay.textContent = `${correct}/${total}`;
    }

    function updateUI() {
        const maxIndex = questionDatabase[currentCategory].length - 1;
        const totalQuestions = questionDatabase[currentCategory].length;
        
        // Update question info
        currentQuestionInfo.textContent = `Question ${currentQuestionIndex + 1} of ${totalQuestions}`;
        
        // Update navigation buttons
        prevButton.disabled = currentQuestionIndex === 0;
        nextButton.disabled = currentQuestionIndex === maxIndex;
        
        // Update progress bar
        const answeredInCategory = userAnswers[currentCategory].filter(answer => answer !== null).length;
        const progressPercentage = (answeredInCategory / totalQuestions) * 100;
        progressBar.value = progressPercentage;
    }

    // Initialize the demo
    init();
});
</script>

</body>
</html>
